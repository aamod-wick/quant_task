{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Importing Dependencies"
      ],
      "metadata": {
        "id": "XD-Gj-s2hDn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#primary packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "from pprint import pprint\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "from google.colab import files\n",
        "\n",
        "#surpress warnings for resmpling and rolling functions\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n"
      ],
      "metadata": {
        "id": "kJ-gjMyMM6qA"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Reading Data"
      ],
      "metadata": {
        "id": "o9q56T29hMsf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload the file \"Data_files.zip\" from your file system. The rest is handled automatically by the below block of code. This code will extract the zip folder and add uit to your file system in collab.  I have created this to ease checking of my code on your side . Kindly upload the zip file directly int  the same format you have provided on my email."
      ],
      "metadata": {
        "id": "ch2U8mJrhPYT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XeBZvaXZu_qd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "Hq8pG8zvMKMo",
        "outputId": "65a550bb-b3be-467d-c540-a74092bda892"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-fd357060-d276-43a1-b88d-e4c8a022800e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-fd357060-d276-43a1-b88d-e4c8a022800e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Data_files.zip to Data_files.zip\n",
            "Folder unzipped successfully!\n"
          ]
        }
      ],
      "source": [
        "uploaded = files.upload()\n",
        "zip_file = 'Data_files.zip'\n",
        "output_folder = 'Data_Aamod'\n",
        "\n",
        "#extract zip file\n",
        "with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "    zip_ref.extractall(output_folder)\n",
        "\n",
        "print(\"Folder unzipped successfully!\")\n",
        "\n",
        "#Path in collab's file system\n",
        "folder_path = 'Data_Aamod/SampleFiles 2'\n",
        "\n",
        "#csv files read into dataframes\n",
        "daily_data = pd.read_csv(folder_path + \"/SampleDayData.csv\")\n",
        "april_19 = pd.read_csv(folder_path + \"/19thAprilSampleData.csv\")\n",
        "april_22 = pd.read_csv(folder_path + \"/22ndAprilSampleData.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checking for empty values"
      ],
      "metadata": {
        "id": "po_6psDSiv-H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below code extracts rows with NaN values for each of the dataframes. The result for this data shows that there are no missing/ NaN values. It is wise to do this before defining the fuctions as data with missing values might require data cleaning and robust functions."
      ],
      "metadata": {
        "id": "iGegVVzljTVx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "empty_indices_daily = np.unique(np.where(daily_data.isnull())[0])\n",
        "empty_indices_intraday1 = np.unique(np.where(april_19.isnull())[0])\n",
        "empty_indices_intraday2 = np.unique(np.where(april_22.isnull())[0])\n",
        "print(len(empty_indices_daily),len(empty_indices_intraday1) ,len(empty_indices_intraday2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-WJOUZhiyTP",
        "outputId": "b38d1b11-8412-4de4-d901-be00fe00dc42"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fetching all ticker/symbol names"
      ],
      "metadata": {
        "id": "0vHxBbZ-iBtT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ticker/ symbol names in daily_data nd intraday_data needs to be i sync. Else data for some of the tickers might need to be dropped. For this case we only need to deal with one ticker that is \"ABC\"."
      ],
      "metadata": {
        "id": "l0rrL9SKjyKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#fetch all stock names for iteration\n",
        "stock_names = daily_data[\"Stock Name\"].unique()\n",
        "\n",
        "#Check numbe rof tickers/symbols\n",
        "if( len(april_22[\"Stock Name\"].unique()) == len(april_19[\"Stock Name\"].unique()) == len(stock_names)):\n",
        "  print(\"DataFrames have same number of tickers\")\n",
        "else:\n",
        "  print(\"Warning number of ticker don't match!!!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Vky8BwRUitE",
        "outputId": "9e0151eb-9469-454c-fe97-4eb7f10ac56b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrames have same number of tickers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Function Defintion"
      ],
      "metadata": {
        "id": "fHwrDY6QiOlD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thhe functions \"calculate_30_day_average_for_stock\" and \"calculate_cumulative_volume_for_stock\" are helper functions for the function \"find_crossovers\". The averages are calculated using rolling means with a minimum window size of 1 (ensures non NaN values for smaller window size). Unnecessary columns are eliminated in each of the functions before returning the processed DataFrame, this has been done to keep the data lightweight and reduce processing time of vector operations used in the main function i.e. \"find crossovers\".\n",
        "\n"
      ],
      "metadata": {
        "id": "s-JvMIDikdMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_30_day_average(daily_data, stock_symbol):\n",
        "    \"\"\"\n",
        "    Calculate the 30-day average volume for a specific stock symbol.\n",
        "    \"\"\"\n",
        "    stock_data = daily_data[daily_data['Stock Name'] == stock_symbol]\n",
        "    avg_volume = stock_data['Volume'].rolling(window=30, min_periods=1).mean() #min_period =1 ensures non-nan values for first few rows\n",
        "    stock_data['30_day_avg'] = avg_volume\n",
        "\n",
        "    #eliminate unnecesary columns\n",
        "    stock_data = stock_data[[\"Date\",\"30_day_avg\"]] #only return necessary columns\n",
        "    return stock_data\n",
        "\n",
        "def calculate_cumulative_volume(intraday_data, stock_symbol, market_open=\"09:15:00\"):\n",
        "    \"\"\"\n",
        "    Calculate cumulative traded volume for a specific stock symbol within a rolling 60-minute window.\n",
        "    \"\"\"\n",
        "    stock_data = intraday_data[intraday_data['Stock Name'] == stock_symbol].copy()\n",
        "\n",
        "    # Combine Date and Time into a datetime column\n",
        "    stock_data['datetime'] = pd.to_datetime(stock_data['Date'] + ' ' + stock_data['Time'])\n",
        "\n",
        "    # Calculate cumulative volume using a rolling 60-minute window\n",
        "    stock_data.set_index('datetime', inplace=True)\n",
        "\n",
        "    #eliminate unnecesary columns\n",
        "    stock_data['cumulative_volume'] = stock_data['Last Traded Quantity'].rolling(pd.Timedelta(minutes=60)).sum()\n",
        "    stock_data.reset_index(inplace=True)\n",
        "    return stock_data\n",
        "\n",
        "def find_crossovers(daily_data, intraday_data, stock_symbol):\n",
        "    \"\"\"\n",
        "    Find the timestamp for a specific stock symbol when cumulative traded volume\n",
        "    first exceeds the 30-day average volume within a 60-minute rolling window.\n",
        "    \"\"\"\n",
        "    # Get 30-day average data for the stock\n",
        "    stock_daily_data = calculate_30_day_average(daily_data, stock_symbol)\n",
        "\n",
        "    # Get intraday data with cumulative volume for the stock\n",
        "    stock_intraday_data = calculate_cumulative_volume(intraday_data, stock_symbol)\n",
        "\n",
        "    # Merge the 30-day average into the intraday data\n",
        "    stock_intraday_data = stock_intraday_data.merge(\n",
        "        stock_daily_data[['Date', '30_day_avg']],\n",
        "        on='Date',\n",
        "        how='left'\n",
        "    )\n",
        "\n",
        "    # Find the first timestamp where cumulative volume exceeds the 30-day average\n",
        "    crossover = stock_intraday_data[stock_intraday_data['cumulative_volume'] > stock_intraday_data['30_day_avg']]\n",
        "    if not crossover.empty:\n",
        "        return crossover.iloc[0]['datetime']\n",
        "    else:\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "OIHd016OYtcB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Result (Crossovers)"
      ],
      "metadata": {
        "id": "UWxXvgPliSuL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prints 2 dictionaries which contain timestamps for crossovers on each day"
      ],
      "metadata": {
        "id": "wxCymDIel43n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Results for 19th and 22nd April\n",
        "results_19th = {}\n",
        "results_22nd = {}\n",
        "\n",
        "# Loop through each stock and process data\n",
        "for stock in stock_names:\n",
        "    results_19th[stock] = find_crossovers(daily_data, april_19, stock)\n",
        "    results_22nd[stock] = find_crossovers(daily_data, april_22, stock)\n",
        "\n",
        "# Output results\n",
        "print(\"Results for 19th April:\")\n",
        "pprint(results_19th, sort_dicts=False)\n",
        "\n",
        "print(\"\\nResults for 22nd April:\")\n",
        "pprint(results_22nd, sort_dicts=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVdG0ms_hds6",
        "outputId": "d52f4ac2-8bc7-4066-cd60-9b1f0bd2be14"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for 19th April:\n",
            "{'ABC': None}\n",
            "\n",
            "Results for 22nd April:\n",
            "{'ABC': Timestamp('2024-04-22 09:41:44')}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results are stored in the csv files i have shared in the mail as well as on github .\n",
        "\n",
        "In JSON format Results for 19th April: {'ABC': None}\n",
        "\n",
        "Results for 22nd April: {'ABC': Timestamp('2024-04-22 09:41:44')}\n",
        "\n",
        "These are the instances for each day where the volume FIRST crossed the rolling window average as requested in the mail .There can be multiple crosses in a day . ON 19th there wasnt a single cross where as on 22nd there was atleast one crossover .\n",
        "\n"
      ],
      "metadata": {
        "id": "xOtdQ6BG2LJV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U8R2Fx9e1MIu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}